{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor modelo que he obtenido es a traves de una red convolucional, pero esta conclusi칩n tiene trampa, ya que los modelos de ML los he entrenado con s칩lo la decima parte del dataset, con lo que la red neuronal parte con ventaja frente al resto. Si bien es cierto que para entrenar una red neuronal se necesitan muchos datos, y con pocos dan resultados mediocres, hay que tenerlo en cuenta.\n",
    "\n",
    "Si fuese Amazon, o Google y tuviese tiempo y potencia de calculo infinito, podr칤a haber entrenado todo con el dataset completo, y realizar distintos ajustes, para obtener unos resultados mejores. En cualquier caso, el objetivo de la practica es aplicar lo ense침ado en clase, y no lograr un accuracy impresionante.\n",
    "\n",
    "Me resulta destacable que, aunque el modelo que mejor se comporta es el de SVM, uno de los mejores modelos es el m치s sencillo, un Bayes. \n",
    "\n",
    "**쯈u칠 puede indicar?**\n",
    "\n",
    "Pues probalemente que al quitar datos el modelo que  podiamos generar se ha resentido. En la primera iteraci칩n, cuando trabajaba con el dataset completo, al usar Bayes he obtenido un `acc=0.77` mientras que con el dataset reducido ha sido de `0.72`.\n",
    "\n",
    "Tambien es cierto que es posible que la selecci칩n de caracteristicas en el vectorizador (y en el stemming) no est칠 optimizada para el problema, y se pudiese trabajar m치s en ella. Por poner un ejemplo, se ve que en el dataset hay caracteres extra침os, procedentes de un [error de decodificaci칩n de utf-8](https://www.i18nqa.com/debug/utf8-debug.html). Esos errores pueden enmascarar caracteres unicodes tipo emojis que pueden tambien ser interesantes en la interpretacion de sentimientos. No es lo mismo usar un 游땕que 游뗴, y cuando he hecho la vectorizaci칩n he ignorado todos esos datos. \n",
    "\n",
    "En cualquier caso aqui tienes una tabla resumen de todos los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RESULT_FILE='results/resultados_modelos.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadosModelo = pd.read_csv(RESULT_FILE)\n",
    "resultadosModelo = resultadosModelo.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>parametros-optimos</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>acc_sen_esp_ppv_fsc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNN - Big Dataset</td>\n",
       "      <td>-</td>\n",
       "      <td>0.927431</td>\n",
       "      <td>0.761541</td>\n",
       "      <td>[[ 7141  3821]\\n [ 2140 11896]]</td>\n",
       "      <td>[0.7615409232738619, 0.847534910230835, 0.6514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM - Small Dataset</td>\n",
       "      <td>{'svm__C': 5.994842503189409, 'svm__gamma': 0....</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>[[ 799  507]\\n [ 294 1400]]</td>\n",
       "      <td>[0.733, 0.8264462809917356, 0.611791730474732,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boosted Tree - Small Dataset</td>\n",
       "      <td>{'boosted__learning_rate': 0.1, 'boosted__max_...</td>\n",
       "      <td>0.714857</td>\n",
       "      <td>0.728667</td>\n",
       "      <td>[[ 784  522]\\n [ 292 1402]]</td>\n",
       "      <td>[0.7286666666666667, 0.8276269185360094, 0.600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayes - Small Dataset</td>\n",
       "      <td>{'vect__analyzer': 'char', 'vect__max_df': 1.0...</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>[[ 909  397]\\n [ 436 1258]]</td>\n",
       "      <td>[0.7223333333333334, 0.7426210153482881, 0.696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CNN - Small Dataset</td>\n",
       "      <td>-</td>\n",
       "      <td>0.992667</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>[[ 682  368]\\n [ 366 1084]]</td>\n",
       "      <td>[0.7064, 0.7475862068965518, 0.649523809523809...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Boosted Tree - Lemma - Small Dataset</td>\n",
       "      <td>{'boosted__learning_rate': 0.1, 'boosted__max_...</td>\n",
       "      <td>0.686955</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>[[ 692  614]\\n [ 282 1412]]</td>\n",
       "      <td>[0.7013333333333334, 0.833530106257379, 0.5298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bayes - Lemma - Small Dataset</td>\n",
       "      <td>{'vect__analyzer': 'word', 'vect__max_df': 0.5...</td>\n",
       "      <td>0.686670</td>\n",
       "      <td>0.690333</td>\n",
       "      <td>[[ 704  602]\\n [ 327 1367]]</td>\n",
       "      <td>[0.6903333333333334, 0.806965761511216, 0.5390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVM - Lemma - Small Dataset</td>\n",
       "      <td>{'svm__C': 1.0, 'svm__gamma': 0.03162277660168...</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>[[ 508  798]\\n [ 147 1547]]</td>\n",
       "      <td>[0.685, 0.9132231404958677, 0.3889739663093415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RNN - Small Dataset</td>\n",
       "      <td>LSTM con Dropout</td>\n",
       "      <td>0.964667</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>[[727 323]\\n [468 982]]</td>\n",
       "      <td>[0.6836, 0.6772413793103448, 0.692380952380952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forestyes - Small Dataset</td>\n",
       "      <td>{'forest__max_depth': 23, 'vect__analyzer': 'c...</td>\n",
       "      <td>0.668857</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>[[ 630  676]\\n [ 329 1365]]</td>\n",
       "      <td>[0.665, 0.8057851239669421, 0.4823889739663093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forestyes - Lemma - Small Dataset</td>\n",
       "      <td>{'forest__max_depth': 28, 'vect__analyzer': 'c...</td>\n",
       "      <td>0.660237</td>\n",
       "      <td>0.664667</td>\n",
       "      <td>[[ 562  744]\\n [ 262 1432]]</td>\n",
       "      <td>[0.6646666666666666, 0.8453364817001181, 0.430...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree - Small Dataset</td>\n",
       "      <td>{'tree__max_depth': 19, 'vect__analyzer': 'wor...</td>\n",
       "      <td>0.653286</td>\n",
       "      <td>0.649667</td>\n",
       "      <td>[[ 479  827]\\n [ 224 1470]]</td>\n",
       "      <td>[0.6496666666666666, 0.8677685950413223, 0.366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree - Lemma - Small Dataset</td>\n",
       "      <td>{'tree__max_depth': 19, 'vect__analyzer': 'wor...</td>\n",
       "      <td>0.650236</td>\n",
       "      <td>0.647333</td>\n",
       "      <td>[[ 362  944]\\n [ 114 1580]]</td>\n",
       "      <td>[0.6473333333333333, 0.9327036599763873, 0.277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RNN B치sico - Small Dataset</td>\n",
       "      <td>Fuera &lt;MENTIONS&gt; y &lt;HASHTAG&gt;</td>\n",
       "      <td>0.989733</td>\n",
       "      <td>0.606000</td>\n",
       "      <td>[[608 442]\\n [543 907]]</td>\n",
       "      <td>[0.606, 0.6255172413793103, 0.579047619047619,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Modelo  \\\n",
       "10                         CNN - Big Dataset   \n",
       "0                        SVM - Small Dataset   \n",
       "4               Boosted Tree - Small Dataset   \n",
       "1                      Bayes - Small Dataset   \n",
       "11                       CNN - Small Dataset   \n",
       "8       Boosted Tree - Lemma - Small Dataset   \n",
       "5              Bayes - Lemma - Small Dataset   \n",
       "9                SVM - Lemma - Small Dataset   \n",
       "13                      RNN - Small Dataset    \n",
       "3           Random Forestyes - Small Dataset   \n",
       "7   Random Forestyes - Lemma - Small Dataset   \n",
       "2              Decision Tree - Small Dataset   \n",
       "6      Decision Tree - Lemma - Small Dataset   \n",
       "12               RNN B치sico - Small Dataset    \n",
       "\n",
       "                                   parametros-optimos     train      test  \\\n",
       "10                                                  -  0.927431  0.761541   \n",
       "0   {'svm__C': 5.994842503189409, 'svm__gamma': 0....  0.716000  0.733000   \n",
       "4   {'boosted__learning_rate': 0.1, 'boosted__max_...  0.714857  0.728667   \n",
       "1   {'vect__analyzer': 'char', 'vect__max_df': 1.0...  0.717000  0.722333   \n",
       "11                                                  -  0.992667  0.706400   \n",
       "8   {'boosted__learning_rate': 0.1, 'boosted__max_...  0.686955  0.701333   \n",
       "5   {'vect__analyzer': 'word', 'vect__max_df': 0.5...  0.686670  0.690333   \n",
       "9   {'svm__C': 1.0, 'svm__gamma': 0.03162277660168...  0.665524  0.685000   \n",
       "13                                   LSTM con Dropout  0.964667  0.683600   \n",
       "3   {'forest__max_depth': 23, 'vect__analyzer': 'c...  0.668857  0.665000   \n",
       "7   {'forest__max_depth': 28, 'vect__analyzer': 'c...  0.660237  0.664667   \n",
       "2   {'tree__max_depth': 19, 'vect__analyzer': 'wor...  0.653286  0.649667   \n",
       "6   {'tree__max_depth': 19, 'vect__analyzer': 'wor...  0.650236  0.647333   \n",
       "12                       Fuera <MENTIONS> y <HASHTAG>  0.989733  0.606000   \n",
       "\n",
       "                   confusion_matrix  \\\n",
       "10  [[ 7141  3821]\\n [ 2140 11896]]   \n",
       "0       [[ 799  507]\\n [ 294 1400]]   \n",
       "4       [[ 784  522]\\n [ 292 1402]]   \n",
       "1       [[ 909  397]\\n [ 436 1258]]   \n",
       "11      [[ 682  368]\\n [ 366 1084]]   \n",
       "8       [[ 692  614]\\n [ 282 1412]]   \n",
       "5       [[ 704  602]\\n [ 327 1367]]   \n",
       "9       [[ 508  798]\\n [ 147 1547]]   \n",
       "13          [[727 323]\\n [468 982]]   \n",
       "3       [[ 630  676]\\n [ 329 1365]]   \n",
       "7       [[ 562  744]\\n [ 262 1432]]   \n",
       "2       [[ 479  827]\\n [ 224 1470]]   \n",
       "6       [[ 362  944]\\n [ 114 1580]]   \n",
       "12          [[608 442]\\n [543 907]]   \n",
       "\n",
       "                                  acc_sen_esp_ppv_fsc  \n",
       "10  [0.7615409232738619, 0.847534910230835, 0.6514...  \n",
       "0   [0.733, 0.8264462809917356, 0.611791730474732,...  \n",
       "4   [0.7286666666666667, 0.8276269185360094, 0.600...  \n",
       "1   [0.7223333333333334, 0.7426210153482881, 0.696...  \n",
       "11  [0.7064, 0.7475862068965518, 0.649523809523809...  \n",
       "8   [0.7013333333333334, 0.833530106257379, 0.5298...  \n",
       "5   [0.6903333333333334, 0.806965761511216, 0.5390...  \n",
       "9   [0.685, 0.9132231404958677, 0.3889739663093415...  \n",
       "13  [0.6836, 0.6772413793103448, 0.692380952380952...  \n",
       "3   [0.665, 0.8057851239669421, 0.4823889739663093...  \n",
       "7   [0.6646666666666666, 0.8453364817001181, 0.430...  \n",
       "2   [0.6496666666666666, 0.8677685950413223, 0.366...  \n",
       "6   [0.6473333333333333, 0.9327036599763873, 0.277...  \n",
       "12  [0.606, 0.6255172413793103, 0.579047619047619,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultadosModelo.sort_values(by=['test'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo lo que ha pasado con la red recurrente, en la que algunas de las asunciones hechas mientras se limpiaba el dataset hacian que el el modelo m치s sencillo no se pudiese entrenar, mucho me temo que para obtener mejores resultados tendr칤a que trabajar m치s en la limpieza del dataset, y una vez limpio volver a repetir todos los modelos para ver si los resultados se mantienen coherentes.\n",
    "\n",
    "**쯈u칠 pasos podriamos usar para limpiar el dataset?**\n",
    "\n",
    "Adem치s de los ya realizados varios:\n",
    "\n",
    "* Eliminar las menciones. \n",
    "* Los hashtags podriamos eliminarlos o simplemente quitarles el # para que sean una palabra\n",
    "* Borrar los emojis o sustituirlos por: 游땏 -> GrinningFaceEmoji...\n",
    "* Eliminar algunas cosas t칤picas de tweets, como poner un punto al principio de una frase, escribir RT\n",
    "* Sustituir los verbos conjugados por su forma simple, has, had -> have. Esto se puede hacer con Spacy\n",
    "* sustituir siglas de interner (LOL, WTF, OMG) y abreviaturas (don't) por palabras completas\n",
    "\n",
    "En definitiva, dedicarle m치s trabajo a el preprocesado y menos a la optimizaci칩n de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
