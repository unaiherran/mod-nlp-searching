{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor modelo que he obtenido es a traves de una red convolucional, pero esta conclusión tiene trampa, ya que los modelos de ML los he entrenado con sólo la decima parte del dataset, con lo que la red neuronal parte con ventaja frente al resto. Si bien es cierto que para entrenar una red neuronal se necesitan muchos datos, y con pocos dan resultados mediocres, hay que tenerlo en cuenta.\n",
    "\n",
    "Si fuese Amazon, o Google y tuviese tiempo y potencia de calculo infinito, podría haber entrenado todo con el dataset completo, y realizar distintos ajustes, para obtener unos resultados mejores. En cualquier caso, el objetivo de la practica es aplicar lo enseñado en clase, y no lograr un accuracy impresionante.\n",
    "\n",
    "Me resulta destacable que, aunque el modelo que mejor se comporta es el de SVM, uno de los mejores modelos es el más sencillo, un Bayes. \n",
    "\n",
    "**¿Qué puede indicar?**\n",
    "\n",
    "Pues probalemente que al quitar datos el modelo que  podiamos generar se ha resentido. En la primera iteración, cuando trabajaba con el dataset completo, al usar Bayes he obtenido un `acc=0.77` mientras que con el dataset reducido ha sido de `0.72`.\n",
    "\n",
    "Tambien es cierto que es posible que la selección de caracteristicas en el vectorizador (y en el stemming) no esté optimizada para el problema, y se pudiese trabajar más en ella. Por poner un ejemplo, se ve que en el dataset hay caracteres extraños, procedentes de un [error de decodificación de utf-8](https://www.i18nqa.com/debug/utf8-debug.html). Esos errores pueden enmascarar caracteres unicodes tipo emojis que pueden tambien ser interesantes en la interpretacion de sentimientos. No es lo mismo usar un 😊que 🙁, y cuando he hecho la vectorización he ignorado todos esos datos. \n",
    "\n",
    "En cualquier caso aqui tienes una tabla resumen de todos los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RESULT_FILE='results/resultados_modelos.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadosModelo = pd.read_csv(RESULT_FILE)\n",
    "resultadosModelo = resultadosModelo.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>parametros-optimos</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>acc_sen_esp_ppv_fsc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNN - Big Dataset</td>\n",
       "      <td>-</td>\n",
       "      <td>0.927431</td>\n",
       "      <td>0.761541</td>\n",
       "      <td>[[ 7141  3821]\\n [ 2140 11896]]</td>\n",
       "      <td>[0.7615409232738619, 0.847534910230835, 0.6514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM - Small Dataset</td>\n",
       "      <td>{'svm__C': 5.994842503189409, 'svm__gamma': 0....</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>[[ 799  507]\\n [ 294 1400]]</td>\n",
       "      <td>[0.733, 0.8264462809917356, 0.611791730474732,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boosted Tree - Small Dataset</td>\n",
       "      <td>{'boosted__learning_rate': 0.1, 'boosted__max_...</td>\n",
       "      <td>0.714857</td>\n",
       "      <td>0.728667</td>\n",
       "      <td>[[ 784  522]\\n [ 292 1402]]</td>\n",
       "      <td>[0.7286666666666667, 0.8276269185360094, 0.600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayes - Small Dataset</td>\n",
       "      <td>{'vect__analyzer': 'char', 'vect__max_df': 1.0...</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>[[ 909  397]\\n [ 436 1258]]</td>\n",
       "      <td>[0.7223333333333334, 0.7426210153482881, 0.696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CNN - Small Dataset</td>\n",
       "      <td>-</td>\n",
       "      <td>0.992667</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>[[ 682  368]\\n [ 366 1084]]</td>\n",
       "      <td>[0.7064, 0.7475862068965518, 0.649523809523809...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Boosted Tree - Lemma - Small Dataset</td>\n",
       "      <td>{'boosted__learning_rate': 0.1, 'boosted__max_...</td>\n",
       "      <td>0.686955</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>[[ 692  614]\\n [ 282 1412]]</td>\n",
       "      <td>[0.7013333333333334, 0.833530106257379, 0.5298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bayes - Lemma - Small Dataset</td>\n",
       "      <td>{'vect__analyzer': 'word', 'vect__max_df': 0.5...</td>\n",
       "      <td>0.686670</td>\n",
       "      <td>0.690333</td>\n",
       "      <td>[[ 704  602]\\n [ 327 1367]]</td>\n",
       "      <td>[0.6903333333333334, 0.806965761511216, 0.5390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVM - Lemma - Small Dataset</td>\n",
       "      <td>{'svm__C': 1.0, 'svm__gamma': 0.03162277660168...</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>[[ 508  798]\\n [ 147 1547]]</td>\n",
       "      <td>[0.685, 0.9132231404958677, 0.3889739663093415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RNN - Small Dataset</td>\n",
       "      <td>LSTM con Dropout</td>\n",
       "      <td>0.964667</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>[[727 323]\\n [468 982]]</td>\n",
       "      <td>[0.6836, 0.6772413793103448, 0.692380952380952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forestyes - Small Dataset</td>\n",
       "      <td>{'forest__max_depth': 23, 'vect__analyzer': 'c...</td>\n",
       "      <td>0.668857</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>[[ 630  676]\\n [ 329 1365]]</td>\n",
       "      <td>[0.665, 0.8057851239669421, 0.4823889739663093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forestyes - Lemma - Small Dataset</td>\n",
       "      <td>{'forest__max_depth': 28, 'vect__analyzer': 'c...</td>\n",
       "      <td>0.660237</td>\n",
       "      <td>0.664667</td>\n",
       "      <td>[[ 562  744]\\n [ 262 1432]]</td>\n",
       "      <td>[0.6646666666666666, 0.8453364817001181, 0.430...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree - Small Dataset</td>\n",
       "      <td>{'tree__max_depth': 19, 'vect__analyzer': 'wor...</td>\n",
       "      <td>0.653286</td>\n",
       "      <td>0.649667</td>\n",
       "      <td>[[ 479  827]\\n [ 224 1470]]</td>\n",
       "      <td>[0.6496666666666666, 0.8677685950413223, 0.366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree - Lemma - Small Dataset</td>\n",
       "      <td>{'tree__max_depth': 19, 'vect__analyzer': 'wor...</td>\n",
       "      <td>0.650236</td>\n",
       "      <td>0.647333</td>\n",
       "      <td>[[ 362  944]\\n [ 114 1580]]</td>\n",
       "      <td>[0.6473333333333333, 0.9327036599763873, 0.277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RNN Básico - Small Dataset</td>\n",
       "      <td>Fuera &lt;MENTIONS&gt; y &lt;HASHTAG&gt;</td>\n",
       "      <td>0.989733</td>\n",
       "      <td>0.606000</td>\n",
       "      <td>[[608 442]\\n [543 907]]</td>\n",
       "      <td>[0.606, 0.6255172413793103, 0.579047619047619,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Modelo  \\\n",
       "10                         CNN - Big Dataset   \n",
       "0                        SVM - Small Dataset   \n",
       "4               Boosted Tree - Small Dataset   \n",
       "1                      Bayes - Small Dataset   \n",
       "11                       CNN - Small Dataset   \n",
       "8       Boosted Tree - Lemma - Small Dataset   \n",
       "5              Bayes - Lemma - Small Dataset   \n",
       "9                SVM - Lemma - Small Dataset   \n",
       "13                      RNN - Small Dataset    \n",
       "3           Random Forestyes - Small Dataset   \n",
       "7   Random Forestyes - Lemma - Small Dataset   \n",
       "2              Decision Tree - Small Dataset   \n",
       "6      Decision Tree - Lemma - Small Dataset   \n",
       "12               RNN Básico - Small Dataset    \n",
       "\n",
       "                                   parametros-optimos     train      test  \\\n",
       "10                                                  -  0.927431  0.761541   \n",
       "0   {'svm__C': 5.994842503189409, 'svm__gamma': 0....  0.716000  0.733000   \n",
       "4   {'boosted__learning_rate': 0.1, 'boosted__max_...  0.714857  0.728667   \n",
       "1   {'vect__analyzer': 'char', 'vect__max_df': 1.0...  0.717000  0.722333   \n",
       "11                                                  -  0.992667  0.706400   \n",
       "8   {'boosted__learning_rate': 0.1, 'boosted__max_...  0.686955  0.701333   \n",
       "5   {'vect__analyzer': 'word', 'vect__max_df': 0.5...  0.686670  0.690333   \n",
       "9   {'svm__C': 1.0, 'svm__gamma': 0.03162277660168...  0.665524  0.685000   \n",
       "13                                   LSTM con Dropout  0.964667  0.683600   \n",
       "3   {'forest__max_depth': 23, 'vect__analyzer': 'c...  0.668857  0.665000   \n",
       "7   {'forest__max_depth': 28, 'vect__analyzer': 'c...  0.660237  0.664667   \n",
       "2   {'tree__max_depth': 19, 'vect__analyzer': 'wor...  0.653286  0.649667   \n",
       "6   {'tree__max_depth': 19, 'vect__analyzer': 'wor...  0.650236  0.647333   \n",
       "12                       Fuera <MENTIONS> y <HASHTAG>  0.989733  0.606000   \n",
       "\n",
       "                   confusion_matrix  \\\n",
       "10  [[ 7141  3821]\\n [ 2140 11896]]   \n",
       "0       [[ 799  507]\\n [ 294 1400]]   \n",
       "4       [[ 784  522]\\n [ 292 1402]]   \n",
       "1       [[ 909  397]\\n [ 436 1258]]   \n",
       "11      [[ 682  368]\\n [ 366 1084]]   \n",
       "8       [[ 692  614]\\n [ 282 1412]]   \n",
       "5       [[ 704  602]\\n [ 327 1367]]   \n",
       "9       [[ 508  798]\\n [ 147 1547]]   \n",
       "13          [[727 323]\\n [468 982]]   \n",
       "3       [[ 630  676]\\n [ 329 1365]]   \n",
       "7       [[ 562  744]\\n [ 262 1432]]   \n",
       "2       [[ 479  827]\\n [ 224 1470]]   \n",
       "6       [[ 362  944]\\n [ 114 1580]]   \n",
       "12          [[608 442]\\n [543 907]]   \n",
       "\n",
       "                                  acc_sen_esp_ppv_fsc  \n",
       "10  [0.7615409232738619, 0.847534910230835, 0.6514...  \n",
       "0   [0.733, 0.8264462809917356, 0.611791730474732,...  \n",
       "4   [0.7286666666666667, 0.8276269185360094, 0.600...  \n",
       "1   [0.7223333333333334, 0.7426210153482881, 0.696...  \n",
       "11  [0.7064, 0.7475862068965518, 0.649523809523809...  \n",
       "8   [0.7013333333333334, 0.833530106257379, 0.5298...  \n",
       "5   [0.6903333333333334, 0.806965761511216, 0.5390...  \n",
       "9   [0.685, 0.9132231404958677, 0.3889739663093415...  \n",
       "13  [0.6836, 0.6772413793103448, 0.692380952380952...  \n",
       "3   [0.665, 0.8057851239669421, 0.4823889739663093...  \n",
       "7   [0.6646666666666666, 0.8453364817001181, 0.430...  \n",
       "2   [0.6496666666666666, 0.8677685950413223, 0.366...  \n",
       "6   [0.6473333333333333, 0.9327036599763873, 0.277...  \n",
       "12  [0.606, 0.6255172413793103, 0.579047619047619,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultadosModelo.sort_values(by=['test'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo lo que ha pasado con la red recurrente, en la que algunas de las asunciones hechas mientras se limpiaba el dataset hacian que el el modelo más sencillo no se pudiese entrenar, mucho me temo que para obtener mejores resultados tendría que trabajar más en la limpieza del dataset, y una vez limpio volver a repetir todos los modelos para ver si los resultados se mantienen coherentes.\n",
    "\n",
    "**¿Qué pasos podriamos usar para limpiar el dataset?**\n",
    "\n",
    "Además de los ya realizados varios:\n",
    "\n",
    "* Eliminar las menciones. \n",
    "* Los hashtags podriamos eliminarlos o simplemente quitarles el # para que sean una palabra\n",
    "* Borrar los emojis o sustituirlos por: 😄 -> GrinningFaceEmoji...\n",
    "* Eliminar algunas cosas típicas de tweets, como poner un punto al principio de una frase, escribir RT\n",
    "* Sustituir los verbos conjugados por su forma simple, has, had -> have. Esto se puede hacer con Spacy\n",
    "* sustituir siglas de interner (LOL, WTF, OMG) y abreviaturas (don't) por palabras completas\n",
    "\n",
    "En definitiva, dedicarle más trabajo a el preprocesado y menos a la optimización de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
